import fs from "fs";
import path from "path";
import {
  createLogger,
  embedText,
  getOsClient,
  fileChecksum,
  runLimited,
  percentile,
  prepareOpensearchIndexName,
  ensureIndex,
  PATH_TO_REPO
} from "./utils.js";

const log = createLogger(import.meta.url);

import { parse as parseJava } from "java-parser";

// Flag to skip cleanup of obsolete files and chunks
const SKIP_CLEANUP = process.env.SKIP_CLEANUP === "true";
// Opensearch index name should end up with repo folder name
const REPO_INDEXER_INDEX_NAME = process.env.REPO_INDEXER_INDEX_NAME || prepareOpensearchIndexName();

// -----------------------------
// BLACKLIST FILENAMES
// -----------------------------
const blackList = [
  "bundle.js",
  "README.md",
  "package-info.java",
  "jquery-slim.min.js",
  "bootstrap.min.js",
  "LICENSE",

];

// -----------------------------
// FILE COLLECTION
// -----------------------------
async function getFiles(dir, exts = [".js", ".ts", ".java", ".py"]) {
  const entries = await fs.promises.readdir(dir, { withFileTypes: true });
  const files = await Promise.all(entries.map(entry => {
    const res = path.resolve(dir, entry.name);
    return entry.isDirectory() ? getFiles(res, exts) : res;
  }));
  return Array.prototype
    .concat(...files)
    .filter(f => exts.includes(path.extname(f)))
    .filter(f => !blackList.includes(path.basename(f)));
}

// -----------------------------
// JAVA METHOD CHUNKING
// -----------------------------
function chunkJavaByMethods(content) {
  try {
    const cst = parseJava(content);
    const lines = content.split(/\r?\n/);
    const chunks = [];

    const methodRegex = /(?:public|protected|private|static|\s)+[\w<>\[\]]+\s+([A-Za-z0-9_]+)\s*\([^)]*\)\s*\{/;
    let chunkIndex = 0;
    let current = null;

    for (let i = 0; i < lines.length; i++) {
      const line = lines[i];
      const m = line.match(methodRegex);
      if (m) {
        if (current) {
          current.end = i - 1;
          const body = lines.slice(current.start, current.end + 1).join("\n");
          chunks.push({
            chunk_id: chunkIndex++,
            function_name: current.name || "anonymous",
            start_line: current.start + 1,
            end_line: current.end + 1,
            text: body
          });
        }
        current = { start: i, name: m[1] || "anonymous" };
      }
    }

    if (current) {
      current.end = lines.length - 1;
      const body = lines.slice(current.start, current.end + 1).join("\n");
      chunks.push({
        chunk_id: chunkIndex++,
        function_name: current.name || "anonymous",
        start_line: current.start + 1,
        end_line: current.end + 1,
        text: body
      });
    }

    if (chunks.length === 0) {
      chunks.push({
        chunk_id: 0,
        function_name: "full_file",
        start_line: 1,
        end_line: lines.length,
        text: content
      });
    }

    return chunks;
  } catch (err) {
    const lines = content.split(/\r?\n/);
    return [{
      chunk_id: 0,
      function_name: "full_file",
      start_line: 1,
      end_line: lines.length,
      text: content
    }];
  }
}

// -----------------------------
// OTHER LANGUAGES CHUNKING
// -----------------------------
function chunkByFunctions(content, language) {
  const lines = content.split(/\r?\n/);
  const chunks = [];
  let current = null;
  const regexes = [];

  if (language === ".js" || language === ".ts") {
    regexes.push(/^function\s+([a-zA-Z0-9_]+)\s*\(/);
    regexes.push(/^[a-zA-Z0-9_]+\s*=\s*\([^)]*\)\s*=>/);
    regexes.push(/^class\s+([A-Za-z0-9_]+)/);
    regexes.push(/^[A-Za-z0-9_]+\([^)]*\)\s*\{/);
  } else if (language === ".py") {
    regexes.push(/^def\s+([A-Za-z0-9_]+)\s*\(/);
    regexes.push(/^class\s+([A-Za-z0-9_]+)/);
  } else {
    regexes.push(/.*/);
  }

  let chunkIndex = 0;
  const closeChunk = (endIdx) => {
    if (!current) return;
    const { start, functionName } = current;
    const body = lines.slice(start, endIdx + 1).join("\n");
    chunks.push({
      chunk_id: chunkIndex++,
      function_name: functionName,
      start_line: start + 1,
      end_line: endIdx + 1,
      text: body
    });
    current = null;
  };

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i];
    let matched = false;
    for (const r of regexes) {
      const m = line.match(r);
      if (m) {
        matched = true;
        closeChunk(i - 1);
        current = { start: i, functionName: m[1] || "anonymous" };
        break;
      }
    }
    if (matched) continue;
    if (i === lines.length - 1) closeChunk(i);
  }

  if (chunks.length === 0) {
    chunks.push({
      chunk_id: 0,
      function_name: "full_file",
      start_line: 1,
      end_line: lines.length,
      text: content
    });
  }

  return chunks;
}

// -----------------------------
// TEST FILE DETECTION
// -----------------------------
function isTestFile(doc) {
  const filename = doc.filename || "";
  const filepath = doc.filepath || "";

  const lowerPath = filepath.toLowerCase();

  if (lowerPath.includes("/test/") || lowerPath.includes("\\test\\")) {
    return true;
  }

  if (filename.endsWith("Test.java") || filename.endsWith("Tests.java")) {
    return true;
  }

  return false;
}

function getJavaPackage(content) {
  const match = content.match(/^\s*package\s+([\w.]+)\s*;/m);
    
    if (match && match[1]) {
        return match[1];
    } else {
        return "default"; // no package declaration found
    }
}

function getJavaImports(content) {
  const withoutComments = content
    .replace(/\/\*[\s\S]*?\*\//g, "") // block comments
    .replace(/\/\/.*$/gm, "");        // line comments

  const importRegex = /^\s*import\s+(static\s+)?([\w.*]+)\s*;/gm;

  const imports = [];
  let match;

  while ((match = importRegex.exec(withoutComments)) !== null) {
    imports.push(
      (match[1] ? "static " : "") + match[2]
    );
  }
  return imports;
}

// -----------------------------
// FILE IMPORTANCE
// -----------------------------
function computeImportance(filepath, content) {
  let score = 1.0;
  const p = filepath.toLowerCase();
  if (p.includes("/src/") || p.includes("/src/main/") || p.includes("/lib/")) score += 1.0;
  if (p.includes("/test/") || p.includes("/spec/") || p.includes("/__tests__/")) score -= 0.5;
  const lines = content.split(/\r?\n/).length;
  if (lines < 200) score += 0.5;
  return Math.max(score, 0.1);
}

// -----------------------------
// GET STORED CHECKSUMS
// -----------------------------
async function getStoredFileChecksum(docId) {
  try {
    const res = await getOsClient().get({ index: REPO_INDEXER_INDEX_NAME, id: docId });
    return res.body?._source?.checksum ?? null;
  } catch {
    return null;
  }
}

// -----------------------------
// OBSOLETE FILE + CHUNK CLEANUP
// -----------------------------
async function cleanupObsoleteFiles(existingFilepaths) {
  const os = getOsClient();
  const existingSet = new Set(existingFilepaths);

  log.info("Starting obsolete file cleanup...");
  const startTS = Date.now();

  let deletedFiles = 0;
  let deletedChunks = 0;
  let searchAfter = null;

  while (true) {
    const res = await os.search({
      index: REPO_INDEXER_INDEX_NAME,
      size: 500,
      body: {
        sort: [{ filepath: "asc" }],
        ...(searchAfter ? { search_after: searchAfter } : {}),
        query: {
          term: { doc_type: "file" }
        }
      }
    });

    const hits = res.body.hits.hits;
    if (hits.length === 0) break;

    for (const hit of hits) {
      const filepath = hit._source.filepath;

      if (!existingSet.has(filepath)) {
        // delete parent file doc
        await os.delete({
          index: REPO_INDEXER_INDEX_NAME,
          id: hit._id
        }).catch(() => {});
        deletedFiles++;

        // delete all chunks for this file
        const delRes = await os.deleteByQuery({
          index: REPO_INDEXER_INDEX_NAME,
          refresh: true,
          body: {
            query: {
              bool: {
                must: [
                  { term: { doc_type: "chunk" } },
                  { term: { filepath } }
                ]
              }
            }
          }
        });

        deletedChunks += delRes.body.deleted || 0;
        log.info(`Deleted obsolete file + chunks: ${filepath}`);
      }
    }

    searchAfter = hits[hits.length - 1].sort;
  }

  log.info(
    `Cleanup finished in ${Date.now() - startTS} ms. Removed files / chunks: ${deletedFiles} / ${deletedChunks}`
  );
}

// -----------------------------
// MAIN INDEXING
// -----------------------------
async function indexRepo(baseDir) {
  let totalChunks = 0;
  let embedCalls = 0;
  let embedLatencies = [];
  let skippedDuplicates = 0;

  log.info("Ensuring OpenSearch index exists...");
  await ensureIndex(REPO_INDEXER_INDEX_NAME, "./data/create-opensearch-index-chunks.json");
  log.info(`Starting indexing of repo at ${baseDir}, using Opensearch index ${REPO_INDEXER_INDEX_NAME}...`);
  const files = await getFiles(baseDir);
  log.info(`Found ${files.length} files to process.`);
  
  const relativeFiles = files.map(f =>
    path.relative(baseDir, f).replace(/\\/g, "/")
  ); 

  const tasks = files.map(f => async () => {
    const relPath = path.relative(baseDir, f).replace(/\\/g, "/");
    const language = path.extname(f);
    const content = await fs.promises.readFile(f, "utf8");
    const fileHash = fileChecksum(content);
    const packageName = language === ".java" ? getJavaPackage(content) : "no_package";
    const imports = language === ".java" ? getJavaImports(content) : [];

    // Skip unchanged files entirely
    const existingFileChecksum = await getStoredFileChecksum(relPath);
    if (existingFileChecksum === fileHash) {
      skippedDuplicates++;
      return; // do not process chunks or embeddings
    }

    // Chunk detection
    let chunks = [];
    if (language === ".java") chunks = chunkJavaByMethods(content);
    else chunks = chunkByFunctions(content, language);

    const methodNames = chunks.map(c => c.function_name);

    const isTestFlag = isTestFile({ filename: path.basename(f), filepath: relPath });
    const importance = computeImportance(relPath, content);

    // Store parent file doc
    await getOsClient().index({
      index: REPO_INDEXER_INDEX_NAME,
      id: relPath,
      body: {
        doc_type: "file",
        is_test: isTestFlag,
        filename: path.basename(f),
        filepath: relPath,
        content: content,
        language,
        package: packageName,
        imports: imports,
        method_names: methodNames,
        checksum: fileHash,
        importance,
        chunks_count: chunks.length
      }
    });

    // Process chunks (we know file is new/changed, so all chunks need embedding)
    for (const chunk of chunks) {
      totalChunks++;
      const chunkId = `${relPath}::chunk_${chunk.chunk_id}`;
      const chunkHash = fileChecksum(chunk.text);

      const t0 = Date.now();
      const embeddings = await embedText(chunk.text);
      embedCalls++;
      embedLatencies.push(Date.now() - t0);

      if (embeddings.length <= 1) {
        await getOsClient().index({
          index: REPO_INDEXER_INDEX_NAME,
          id: chunkId,
          body: {
            doc_type: "chunk",
            is_test: isTestFlag,
            filename: path.basename(f),
            filepath: relPath,
            language,
            package: packageName,
            imports: imports,
            chunk_id: chunk.chunk_id,
            function_name: chunk.function_name,
            start_line: chunk.start_line,
            end_line: chunk.end_line,
            content: chunk.text,
            checksum: chunkHash,
            embedding: embeddings[0],
            importance
          }
        });
      } else {
        for (let i = 0; i < embeddings.length; i++) {
          const subId = `${chunkId}::sub_${i}`;
          await getOsClient().index({
            index: REPO_INDEXER_INDEX_NAME,
            id: subId,
            body: {
              doc_type: "chunk",
              is_test: isTestFlag,
              filename: path.basename(f),
              filepath: relPath,
              language,
              package: packageName,
              imports: imports,
              chunk_id: chunk.chunk_id,
              sub_idx: i,
              function_name: chunk.function_name,
              start_line: chunk.start_line,
              end_line: chunk.end_line,
              content: chunk.text,
              checksum: chunkHash,
              embedding: embeddings[i],
              importance
            }
          });
        }
      }
    }
  });

  log.info(`Prepared ${tasks.length} tasks. Starting indexing...`);
  const started = Date.now();
  await runLimited(tasks, 4);
  const duration = Date.now() - started;

  embedLatencies.sort((a, b) => a - b);
  const min = embedLatencies[0] ?? 0;
  const max = embedLatencies[embedLatencies.length - 1] ?? 0;
  const p50 = percentile(embedLatencies, 50);

  log.info(
    "\n===== INDEXING STATS =====" +
    `\nChunks processed:         ${totalChunks}` +
    `\nEmbedding calls:          ${embedCalls}` +
    `\nDuplicates skipped:       ${skippedDuplicates}` +
    `\nMin latency:              ${min} ms` +
    `\nMax latency:              ${max} ms` +
    `\nP50 latency:              ${p50} ms` +
    `\nTotal indexing time:      ${duration > 1000 ? (duration/1000 + ' sec') : (duration + ' ms')}` +
    `\n==========================`
  );

  if (SKIP_CLEANUP) {
    log.info(`Skipping obsolete file cleanup as per configuration: SKIP_CLEANUP is ${SKIP_CLEANUP}`);
    return;
  } else {
    await cleanupObsoleteFiles(relativeFiles);
  }
}

// run
await indexRepo(PATH_TO_REPO);
